[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Alejandro, a Data Scientist and Machine Learning Engineer with experience in the finance sector.\nMy academic background includes a Bachelor of Science in Computer Science, a Big Data minor that I had the opportunity to study in the Netherlands and a Master’s degree in Artificial Intelligence.\nFor the last 5 years I have worked on projects related to finance, researching currency overlay solutions and developing advanced trading models. I have also worked utilizing quantum-inspired algorithms to solve well-known financial problems like Index Tracking and arbitrage strategies.\nIn this blog I share some thoughts and ideas that I think are interesting."
  },
  {
    "objectID": "posts/2021_07_29_dollar_index_checkpoint.html",
    "href": "posts/2021_07_29_dollar_index_checkpoint.html",
    "title": "Using Python to construct dollar indices",
    "section": "",
    "text": "In this post we review the methodology for constructing 2 dollar indices. We also code simple procedures to obtain those indices programmatically. Finally, we propose a method to compute a USD dollar index using PCA.\n# dependencies we are going to need\nimport investpy\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nfrom functools import reduce\nfrom typing import Dict, Union\n#hide\nplt.style.use('https://gist.githubusercontent.com/Xylambda/4521dc6404594a715bbc7b75e8c1e2e1/raw/f4f466b27955169abd934428ed98c2be2e1201f2/bpyplot')"
  },
  {
    "objectID": "posts/2021_07_29_dollar_index_checkpoint.html#introduction",
    "href": "posts/2021_07_29_dollar_index_checkpoint.html#introduction",
    "title": "Using Python to construct dollar indices",
    "section": "Introduction",
    "text": "Introduction\nIn order to track the strength of the dollar, different organizations have developed different techniques to create their own dollar indexes. In this post we are going to see 2 methodologies to construct dollar indexes plus an extra one with PCA, though we will not discuss the results (at least in this post).\nThe most common way of constructing an index is by weighting the currency to track against a set of ponderated currencies. The currencies basket, the way weights are chosen and how we aggregate the different ponderated currencies define how we construct an index."
  },
  {
    "objectID": "posts/2021_07_29_dollar_index_checkpoint.html#dollar-indices",
    "href": "posts/2021_07_29_dollar_index_checkpoint.html#dollar-indices",
    "title": "Using Python to construct dollar indices",
    "section": "Dollar indices",
    "text": "Dollar indices\n\nUSDIDX\nThe dollar index is an index that measures the U.S. dollar value. It is built using a geometric mean of the following currencies with their respective weights: * Euro (EUR): 57.6 % weight * Japanese yen (JPY): 13.6 % weight. * Pound sterling (GBP): 11.9 % weight. * Canadian dollar (CAD): 11.9 % weight. * Swedish krona (SEK): 4.2 % weight. * Swiss franc (CHF): 3.6 % weight.\nIn the case of the main U.S. dollar index the weights remain static, which does not allow the index to capture well some market conditions. Furthermore, we see the Swedish Krona taking a relatively high weight, which is certainly outdated in the current market context.\n\n\nTrade weighted U.S. dollar index\nIn 1998, the staff of the Federal Reserve Board introduced a new set of indices, among which was the trade weighted dollar index.\nAlthough the dollar index is not well suited to capture rapid market movements (among other things), the trade weighted U.S. dollar index is not designed to fix this issue but to measure the strenght of the U.S. economy with respect to its trading partners.\nAccording to Loretan [1], the Federal Reserve Board’s nominal dollar index at time \\(t\\) can be defined as \\[\nI_{t} = I_{t-1} \\cdot \\prod_{j=1}^{N(t)} \\left( \\dfrac{e_{j,t}}{e_{j,t-1}} \\right)^{w_{j,t}}\n\\]\nLet’s break the formula to understand each one of the terms: * \\(I_{t-1}\\) is the value of the index at time \\(t-1\\). * \\(e_{j,t}\\) and \\(e_{j,t-1}\\) are the prices of the U.S. dollar in terms of foreign currency \\(j\\) at times \\(t\\) and \\(t-1\\). * \\(w_{j,t}\\) is the weight of currency \\(j\\) at time \\(t\\). * \\(N(t)\\) is the number of foreign currencies in the index at time \\(t\\). * \\(\\sum_{j}w_{j,t} = 1\\).\nNotice how the index is computed using a geometric weighted average. As Loretan explains in [1], this is done because geometric averaging forces proportionately equal appreciation and depreciation of currencies to have the same numerical effect.\nThe weights \\(w_{j,t}\\) are computed as\n\\[\nw_{j,t} = \\frac{1}{2} \\mu_{US,j,t} + \\frac{1}{2} \\left( \\frac{1}{2} \\epsilon_{US, j,t} + \\frac{1}{2} \\tau_{US,jt} \\right)\n\\]\nwhich is nothing more than a linear combination of three submeasures of the degree of trade competition.\nIn the formula above, we can identify some terms that need further explanation:\n\n\\(\\mu_{US,j,t} = M_{US,j,t} / \\sum_{j=1}^{N(t)}M_{US, j,t}\\) is the economy’s bilateral import weight during period \\(t\\). Here, \\(M_{US,j,t}\\) represents the merchandise imports from economy \\(j\\) to the USA in year \\(t\\).\n\\(\\epsilon_{US, j,t} = X_{US,j,t} / \\sum_{j=1}^{N(t)}X_{US,j,t}\\) accounts for the US bilateral export share, where \\(X_{US,j,t}\\) represents the merchandise exports from the USA to economy \\(j\\) in year \\(t\\).\n\\(\\tau_{US,j,t} = \\sum_{k \\neq j, k \\neq US}^{N(t)} \\epsilon_{US, j,t} \\cdot \\mu_{k,j,t} / (1 - \\mu_{j,j,t})\\) measures a form of competition where US-produced goods may also compete with goods produced in economy \\(j\\) if the USA and economy \\(j\\) both export goods to buyers in third-market economies. Here, \\(\\mu_{k,j,t}\\) is the fraction of economy \\(k\\)’s merchandise imports from country \\(j\\) in year \\(t\\). The factor \\(1 / (1 - \\mu_{j,j,t})\\) is used to ensure weights sum up to 1.\n\nFortunately for us, we don’t have to manually compute the weights since they can be found at the Federal Reserve Board [4]."
  },
  {
    "objectID": "posts/2021_07_29_dollar_index_checkpoint.html#constructing-dollar-indices-in-python",
    "href": "posts/2021_07_29_dollar_index_checkpoint.html#constructing-dollar-indices-in-python",
    "title": "Using Python to construct dollar indices",
    "section": "Constructing dollar indices in Python",
    "text": "Constructing dollar indices in Python\n\nUS dollar index\nWe first start with the US dollar index. The construction is pretty simple since it is just a geometric weighted mean. The formula to compute the index is: \\[\nUSDIDX = 50.14348112 \\cdot EURUSD^{-0.576} \\cdot USDJPY^{0.136} \\cdot GBPUSD^{-0.119} \\cdot USDCAD^{0.091} \\cdot USDSEK^{0.042} \\cdot USDCHF^{0.036}\n\\]\nNotice how the sign of the exponent changes to account for the direction in which the currencies are expressed: if the USD is the base currency, the exponent is positive.\nThe number 50.14348112 is a correction factor to force the index to start at value 100.\n\n#hide\ncrosses = [\n    'AUD/USD', # Australia\n    'USD/ARS', # Argentina\n    'USD/BRL', # Brazil\n    'USD/CAD', # Canada\n    'USD/CNY', # China\n    'USD/CLP', # Chile\n    'USD/COP', # Colombia\n    'USD/HKD', # Hong Kong\n    'USD/IDR', # Indonesia\n    'USD/INR', # India\n    'USD/ILS', # Israel\n    'USD/JPY', # Japan\n    'USD/KRW', # Korea\n    'USD/MYR', # Malaysia\n    'USD/MXN', # Mexico\n    'USD/PHP', # Philippines\n    'USD/RUB', # Russia\n    'USD/SAR', # Saudi Arabia\n    'USD/SEK', # Sweeden\n    'USD/SGD', # Singapore\n    'USD/CHF', # Switzerland\n    'USD/TWD', # Taiwan\n    'USD/THB', # Thailand\n    'GBP/USD', # United Kingdom\n    'USD/VND', # vietnam\n    'EUR/USD' # Euro/Area\n]\n\n# download\ncrosses_dict = {}\nfor cross in crosses:\n    data = investpy.get_currency_cross_historical_data(\n        currency_cross=cross,\n        from_date='01/01/1950',\n        to_date='01/01/2022'\n    )\n    \n    crosses_dict[cross] = data[['Open', 'High', 'Low', 'Close']]#.loc['1988-03-04':]\n\n\n#collapse-show\ndef dollar_index(\n    currencies: Dict[str, pd.DataFrame], \n    weights: Dict[str, float]\n) -&gt; Union[pd.DataFrame, pd.Series]:\n    \"\"\"Compute the main U.S. dollar index.\n    \n    Weights must account for the currency cross direction.\n    \n    Parameters\n    ---------\n    currencies : dict\n        Dictionary containing the currency prices.\n    weights : dict\n        Dictionary containing the weights of each currency.\n        \n    Returns\n    -------\n    idx : pandas.DataFrame\n        U.S. dollar index.\n    \"\"\"\n    new = {}\n    \n    idx_crosses = ['EUR/USD', 'USD/JPY', 'GBP/USD', 'USD/CAD', 'USD/SEK', 'USD/CHF']\n    \n    # ponderate each currency\n    for key in idx_crosses:\n        new[key] = currencies[key] ** weights[key]\n        \n    # multiply all currencies\n    idx = reduce(\n        lambda a, b: a.multiply(b),\n        [new[key] for key in new.keys()]\n    )\n    \n    # add correction factor\n    idx *= 50.14348112\n    \n    return idx\n\n\n# create weights dictionary\nidx_crosses = ['EUR/USD', 'USD/JPY', 'GBP/USD', 'USD/CAD', 'USD/SEK', 'USD/CHF']\ndollar_idx_weights = dict(zip(idx_crosses, [-0.576, 0.136, -0.119, 0.091, 0.042, 0.036]))\n\n# compute the us dollar index\nusdidx = dollar_index(crosses_dict, dollar_idx_weights)\nusdidx['Close'].plot(figsize=(15,7), title='USD IDX')\n\nfindfont: Font family ['Franklin Gothic Book'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Franklin Gothic Book'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n\n\n\n\n\n\nTrade weighted dollar index\nIn order to get the weights quickly and in a simple way, we can just copy the table from [4] and use Pandas to read our clipboard.\n\n#collapse-show\nweights = pd.read_clipboard()\nweights.set_index('Country or Region', inplace=True)\nweights.index = crosses + ['Total'] # put currency names instead of country names\n\n\nweights\n\n\n\n\n\n\n\n\n2021\n2020\n2019\n2018\n2017\n2016\n2015\n2014\n2013\n2012\n2011\n2010\n2009\n2008\n2007\n2006\n\n\n\n\nAUD/USD\n1.401\n1.401\n1.401\n1.416\n1.453\n1.443\n1.539\n1.557\n1.596\n1.749\n1.674\n1.549\n1.639\n1.480\n1.364\n1.309\n\n\nUSD/ARS\n0.442\n0.442\n0.442\n0.495\n0.550\n0.524\n0.510\n0.479\n0.530\n0.507\n0.513\n0.469\n0.447\n0.456\n0.380\n0.359\n\n\nUSD/BRL\n1.932\n1.932\n1.932\n1.952\n2.010\n1.903\n2.080\n2.338\n2.426\n2.428\n2.448\n2.194\n2.051\n2.114\n1.857\n1.779\n\n\nUSD/CAD\n13.337\n13.337\n13.337\n13.467\n13.685\n13.873\n14.062\n15.120\n15.513\n15.645\n15.883\n16.078\n15.844\n17.406\n18.089\n18.613\n\n\nUSD/CNY\n13.719\n13.719\n13.719\n15.767\n16.014\n15.635\n15.862\n15.645\n15.564\n15.099\n14.798\n14.848\n14.035\n13.009\n12.839\n12.326\n\n\nUSD/CLP\n0.640\n0.640\n0.640\n0.642\n0.633\n0.621\n0.651\n0.637\n0.706\n0.731\n0.701\n0.606\n0.611\n0.610\n0.586\n0.600\n\n\nUSD/COP\n0.615\n0.615\n0.615\n0.624\n0.598\n0.610\n0.653\n0.704\n0.661\n0.670\n0.659\n0.650\n0.678\n0.666\n0.591\n0.552\n\n\nUSD/HKD\n1.330\n1.330\n1.330\n1.438\n1.489\n1.433\n1.420\n1.450\n1.418\n1.314\n1.349\n1.317\n1.332\n1.233\n1.246\n1.239\n\n\nUSD/IDR\n0.665\n0.665\n0.665\n0.669\n0.678\n0.668\n0.698\n0.726\n0.762\n0.737\n0.770\n0.747\n0.699\n0.679\n0.616\n0.597\n\n\nUSD/INR\n2.869\n2.869\n2.869\n2.800\n2.674\n2.627\n2.458\n2.310\n2.264\n2.228\n2.220\n2.120\n2.069\n1.917\n1.746\n1.499\n\n\nUSD/ILS\n0.985\n0.985\n0.985\n1.037\n1.051\n1.122\n1.149\n1.138\n1.145\n1.154\n1.229\n1.183\n1.219\n1.257\n1.209\n1.168\n\n\nUSD/JPY\n6.377\n6.377\n6.377\n6.282\n6.383\n6.498\n6.359\n6.681\n7.072\n7.568\n7.191\n7.501\n7.263\n7.931\n8.340\n9.065\n\n\nUSD/KRW\n3.283\n3.283\n3.283\n3.273\n3.325\n3.319\n3.400\n3.347\n3.333\n3.264\n3.329\n3.278\n3.044\n2.937\n2.961\n3.076\n\n\nUSD/MYR\n1.278\n1.278\n1.278\n1.232\n1.261\n1.294\n1.229\n1.170\n1.127\n1.115\n1.198\n1.310\n1.310\n1.402\n1.472\n1.752\n\n\nUSD/MXN\n13.698\n13.698\n13.698\n13.452\n13.189\n13.341\n13.331\n12.867\n12.610\n12.261\n11.787\n11.604\n10.988\n10.686\n10.899\n11.281\n\n\nUSD/PHP\n0.662\n0.662\n0.662\n0.644\n0.642\n0.625\n0.601\n0.610\n0.608\n0.626\n0.613\n0.619\n0.615\n0.651\n0.672\n0.717\n\n\nUSD/RUB\n0.468\n0.468\n0.468\n0.514\n0.523\n0.462\n0.509\n0.699\n0.697\n0.692\n0.674\n0.591\n0.636\n0.761\n0.647\n0.635\n\n\nUSD/SAR\n0.511\n0.511\n0.511\n0.482\n0.562\n0.641\n0.694\n0.649\n0.732\n0.719\n0.592\n0.559\n0.664\n0.564\n0.529\n0.423\n\n\nUSD/SEK\n0.559\n0.559\n0.559\n0.549\n0.541\n0.543\n0.554\n0.576\n0.560\n0.607\n0.658\n0.709\n0.770\n0.774\n0.794\n0.829\n\n\nUSD/SGD\n1.891\n1.891\n1.891\n1.870\n1.681\n1.613\n1.569\n1.447\n1.517\n1.696\n1.719\n1.757\n1.692\n1.585\n1.685\n1.698\n\n\nUSD/CHF\n2.844\n2.844\n2.844\n2.632\n2.752\n2.599\n2.452\n2.400\n2.383\n2.295\n2.227\n2.362\n2.504\n2.166\n1.920\n1.823\n\n\nUSD/TWD\n2.145\n2.145\n2.145\n1.940\n1.941\n2.018\n2.010\n2.024\n2.008\n2.073\n2.285\n2.322\n2.055\n2.176\n2.358\n2.469\n\n\nUSD/THB\n1.088\n1.088\n1.088\n1.052\n1.065\n1.075\n1.053\n1.023\n1.030\n1.025\n1.014\n1.043\n1.019\n1.049\n1.063\n1.121\n\n\nGBP/USD\n5.416\n5.416\n5.416\n5.449\n5.291\n5.406\n5.526\n5.287\n5.107\n5.272\n5.338\n5.511\n6.082\n6.023\n6.150\n6.002\n\n\nUSD/VND\n1.761\n1.761\n1.761\n1.350\n1.322\n1.339\n1.148\n0.934\n0.806\n0.703\n0.642\n0.609\n0.593\n0.491\n0.413\n0.333\n\n\nEUR/USD\n20.086\n20.086\n20.086\n18.972\n18.685\n18.769\n18.481\n18.182\n17.824\n17.822\n18.493\n18.464\n20.141\n19.976\n19.575\n18.735\n\n\nTotal\n100.000\n100.000\n100.000\n100.000\n100.000\n100.000\n100.000\n100.000\n100.000\n100.000\n100.000\n100.000\n100.000\n100.000\n100.000\n100.000\n\n\n\n\n\n\n\n\n#collapse-show\ndef trade_weighted_usdidx(currencies: Dict[str, pd.DataFrame], weights: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute trade weighted dollar index.\n    \n    Parameters\n    ----------\n    currencies : dict\n        Dictionary containing all currency crosses.\n    weights : pandas.DataFrame\n        DataFrame containing the weights for each cross. The columns\n        should be years while the index should be the crosses.\n        \n    Returns\n    -------\n    idx : pd.DataFrame\n        Trade weighted dollar index.\n    \"\"\"\n    ponderated_crosses = {}\n    \n    for cross in currencies.keys():\n        \n        ponderations_per_year = []\n        \n        for year in weights.columns:\n            # invert prices if needed\n            if not cross.startswith('USD'):\n                prices = (1 / currencies[cross]).loc[str(year)]\n            else:\n                prices = currencies[cross].loc[str(year)]\n                \n            returns = prices / prices.shift(1)\n            weight = weights.loc[cross, year] / 100\n            \n            ponderations_per_year.append(returns ** weight)\n        \n        ponderated_crosses[cross] = pd.concat(ponderations_per_year)\n            \n    # multiply all currencies\n    idx = reduce(\n        lambda a, b: a.multiply(b),\n        [ponderated_crosses[key] for key in ponderated_crosses.keys()]\n    )\n    \n    idx.iloc[0, :] = 100\n    \n    return idx.cumprod()\n\nSince we could not retrieve the USD/SAR prices for the year 2021, we will only compute the index from 2006 to 2020.\n\nfed_idx = trade_weighted_usdidx(currencies=crosses_dict, weights=weights.drop('2021', axis=1))\nfed_idx.plot(figsize=(15,7), title='Trade weighted dollar index')\n\n\n\n\n\n\n\n\n\n\nBuild your own index\nNote: this idea was inspired by [2] and [6].\nIn this section, we will code a simple index using PCA to compute the weights of the currencies. The idea is to reduce the space into 1 component and then grab the loadings (projected eigenvalues) to use them as weights.\nThe ideal procedure will compute PCA using a rolling window, but I am gonna leave that as an exercise for the reader ;).\n\n#collapse\ncloses = []\nfor cr in crosses_dict.keys():\n    closes.append(crosses_dict[cr].loc[:, 'Close'])\n    \ncloses = pd.concat(closes, axis=1)\ncloses.columns = crosses_dict.keys()\n\nWe will apply PCA over the logarithmic daily returns.\n\nreturns = closes.apply(np.log).diff(1)\npca = PCA(n_components=1)\npca.fit(returns.dropna())\n\nPCA(n_components=1)\n\n\nNow, Scikit-Learn stores an eigenvector, for each principal component, for the projection space. We can use these values as our weights. Below there is a plot showing the values for these weights.\n\nweights_pca = pd.Series(index=returns.columns, data=pca.components_[0])\nweights_pca.plot.bar(figsize=(15,7), title='Weights')\n\n\n\n\n\n\n\n\nNotice how PCA is able to capture the sign of the returns: if the direction is XXX/USD, the weights are negative. Another thing to take into account is the fact that, generally, the crosses with the lowest weights are pegged currencies (USD/CNY, USD/HKD, USD/SAR, USD/VND).\nGoing back to the procedure, since the PCA weights do not sum up to 1, we have to normalize them but taking into account the sign:\n\nweights_pca = (weights_pca / weights_pca.abs().sum())\nweights_pca.plot.bar(figsize=(15,7), title='Fixed Weights')\n\n\n\n\n\n\n\n\n\n#collapse-show\ndef pca_dollar_index(\n    currencies: Dict[str, pd.DataFrame], \n    weights: Dict[str, float]\n) -&gt; Union[pd.DataFrame, pd.Series]:\n    \"\"\"Compute dollar index using PCA.\n    \n    Weights must account for the currency cross direction.\n    \n    Parameters\n    ---------\n    currencies : dict\n        Dictionary containing the currency prices.\n    weights : dict\n        Dictionary containing the weights of each currency.\n        \n    Returns\n    -------\n    idx : pandas.DataFrame\n        U.S. dollar index.\n    \"\"\"\n    new = {}\n    \n    # ponderate each currency\n    for key in idx_crosses:\n        new[key] = currencies[key] ** weights[key]\n        \n    # multiply all currencies\n    idx = reduce(\n        lambda a, b: a.multiply(b),\n        [new[key] for key in new.keys()]\n    )\n    \n    norm_factor = 100 / idx['Close'].dropna().iloc[0]\n    idx *= norm_factor\n    \n    return idx\n\nThe operation norm_factor = 100 / idx['Close'].dropna().iloc[0] is used to force the first day to be 100, as in the other indices.\n\npca_index = pca_dollar_index(crosses_dict, weights_pca)\npca_index.plot(figsize=(15,7), title='PCA Index')\n\n\n\n\n\n\n\n\nPutting all together we can see how the indices evolve:\n\n#collapse\nall_indices = pd.concat(\n    [usdidx['Close'], fed_idx['Close'], pca_index['Close']],\n    axis=1\n)\nall_indices.columns = ['USD IDX', 'FED IDX', 'PCA IDx']\nall_indices.plot(figsize=(15,7))\n\n\n\n\n\n\n\n\nAnd their correlations\n\n#collapse-hide\nplt.figure(figsize=(14,8))\nsns.heatmap(all_indices.corr(), annot=True, annot_kws={'fontsize':20})\n\nfindfont: Font family ['Franklin Gothic Book'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n\n\n\n\nAs expected, all indices are highly correlated, which means they have a strong linear codependence."
  },
  {
    "objectID": "posts/2021_07_29_dollar_index_checkpoint.html#conclusions",
    "href": "posts/2021_07_29_dollar_index_checkpoint.html#conclusions",
    "title": "Using Python to construct dollar indices",
    "section": "Conclusions",
    "text": "Conclusions\nIn this post we’ve reviewed 3 ways of computing a dollar index: * USDIDX: the most common dollar index.\n\nTrade weighted dollar index: an index designed by the USA Federal Reserve to measure how well is USA performing in relation to its trading parters.\nPCA index: based on [2] and [6], which is an index whose weights have been computed depending on the eigenvalues of the covariance matrix of the closing prices returns.\n\nThe construction of the indices is not perfect in the sense that they don’t replicate the true values of the indices. This is due to the lack of data on some days plus a difference in the prices caused by the subset of the market that was used to get the prices not being the same (probably).\nAn additional analysis will have to be carried out to check which is index is better, but that is out of the scope of this post. You can check [7] to have an idea of how to evaluate the goodness of an index, though it will depend on the use case."
  },
  {
    "objectID": "posts/2021_07_29_dollar_index_checkpoint.html#references",
    "href": "posts/2021_07_29_dollar_index_checkpoint.html#references",
    "title": "Using Python to construct dollar indices",
    "section": "References",
    "text": "References\n\n[1] Mico Loretan - Indexes of the Foreign Exchange Value of the Dollar\n[2] Musa Essayyad, Khaled Albinali and Omar Al-Titi - Constructing an alternative dollar index to gauge the movements in currency markets\n[3] The Ice - U.S. Dollar Index Contracts\n[4] Federal Reserve Board - Total Trade Weights\n[5] S&P Dow Jones Indices: Index Methodology\n[6] Yao Lei Xu - Stock Market Analytics with PCA.\n[7] Cerno Capital - Is the US Dollar Index Fit for Purpose"
  },
  {
    "objectID": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html",
    "href": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html",
    "title": "Online beta estimation with the Kalman Filter",
    "section": "",
    "text": "In this post we briefly review the Kalman Filter and we use it to compute the beta between 2 stock returns using Python, though the procedure can also be used in many other fields with an appropiate translation.\n#hide\nimport warnings\nwarnings.filterwarnings('ignore')\n# dependencies we are going to need\nimport investpy\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom kalmanfilter import KalmanFilter\nfrom sklearn.linear_model import LinearRegression\n#hide\nplt.rcParams[\"font.family\"] = \"Times New Roman\""
  },
  {
    "objectID": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#introduction",
    "href": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#introduction",
    "title": "Online beta estimation with the Kalman Filter",
    "section": "Introduction",
    "text": "Introduction\nThe Kalman Filter is an optimal estimation algorithm capable of finding the true state of a signal given that this signal is noisy and/or incomplete. There are many applications where the Kalman filter is an appropiate tool; in this post we see how we can mix the Kalman filter with a linear regression to dynamically compute the beta between 2 stocks."
  },
  {
    "objectID": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#brief-review",
    "href": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#brief-review",
    "title": "Online beta estimation with the Kalman Filter",
    "section": "Brief review",
    "text": "Brief review\nThe Kalman assumes a dynamic linear system given by two equations \\[\n\\begin{matrix}\nx_{k} = A_{k} x_{k} + B u_{k} + q_{k} \\\\\nz_{k} = H_{k} x_{k} + r_{k}\n\\end{matrix}\n\\]\nThe first equation determines the expression that gives the true state of the system. The second equation tells us that any observation \\(z\\) at time \\(k\\) can be expressed as a linear combination of the state \\(x_{k}\\) using the observation model \\(H_{k}\\) plus some noise \\(r_{k}\\).\nThe Kalman Filter estimates the true estate using a feedback control-loop of 2 stages: predict and update.\n\nPredict step \\[\n\\begin{matrix}\n\\hat{x}_{k}^{-} = A_{k} \\hat{x}_{k-1}^{-} + B_{k} u_{k-1} \\\\\nP_{k}^{-} = A_{k}P_{k-1}A_{k}^{T} + Q_{k}\n\\end{matrix}\n\\]\nUpdate step \\[\n\\begin{matrix}\nK_k = P_{k}^{-} H_{k}^{T} (H_{k} P_{k}^{-} H_{k}^{T} + R_{k})^{-1} \\\\\n\\hat{x}_{k} = \\hat{x}_{k}^{-} + K_k (z_k - H_{k} \\hat{x}_{k}^{-}) \\\\\nP_k = (I - K_k H) P_{k}^{-}\n\\end{matrix}\n\\]\n\nWe won’t cover the mathematical derivation of the filter since it is very complex and it would only add useless noise to the post, but you can read it at [3] and [4]."
  },
  {
    "objectID": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#linear-regression-kalman-filtering",
    "href": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#linear-regression-kalman-filtering",
    "title": "Online beta estimation with the Kalman Filter",
    "section": "Linear regression & Kalman filtering",
    "text": "Linear regression & Kalman filtering\nRecall the typical linear regression equation \\[\n\\hat{Y} = \\hat{\\beta}_{0} + \\sum_{j=0}^{p} X_{j} \\hat{\\beta}_{j}\n\\]\nwhich is equivalent to \\[\n\\hat{Y} = X^{T} \\hat{\\beta}\n\\]\nif we include the constant variable 1 in \\(X\\), include \\(\\hat{\\beta}_{0}\\) in the vector of coefficients \\(\\hat{\\beta}\\) and then write the linear model in vector form as an inner product, where \\(X^{T}\\) denotes vector or matrix transpose (\\(X\\) is a column vector) [5].\nFor two dimensions, we can express the above formula as \\[\n\\hat{Y} = (1, x_{j})\\begin{pmatrix} \\hat{\\beta}_{0} \\\\ \\hat{\\beta}_{1} \\end{pmatrix}\n\\]\nThe equation above is similar to the observation equation of the Kalman Filter \\(z_{k} = H_{k} x_{k} + r_{k}\\) and so we can write the following expression \\[\nz_{k} = (1, H_{k})\\begin{pmatrix} x_{k,0} \\\\ x_{k,1} \\end{pmatrix} + r_{k}\n\\]\nThis is telling us that we can plug one stock in the \\(z_{k}\\) and the other in the \\(H_{k}\\) to compute a dynamic beta between both stocks. \\(x_{k,0}\\) would be our alpha and \\(x_{k,1}\\) our beta."
  },
  {
    "objectID": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#the-code",
    "href": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#the-code",
    "title": "Online beta estimation with the Kalman Filter",
    "section": "The code",
    "text": "The code\nWe first download the stock prices of Amazon and Microsoft and compute their arithmetic relative returns. We use arithmetic returns because of the difference in scale of absolute returns.\n\n#collapse-hide\namazon = investpy.get_stock_historical_data(\n    stock='AMZN',\n    country='United States',\n    from_date='01/01/1910',\n    to_date='28/08/2021'\n)\n\n\nmicrosoft = investpy.get_stock_historical_data(\n    stock='MSFT',\n    country='United States',\n    from_date='01/01/1910',\n    to_date='28/08/2021'\n)\n\nama_returns = amazon['Close'].pct_change()\nmic_returns = microsoft['Close'].pct_change()\n\nprices = pd.concat([amazon['Close'], microsoft['Close']], axis=1).dropna()\nprices.columns = ['Amazon', 'Microsoft']\n\nreturns = pd.concat([ama_returns, mic_returns], axis=1).dropna()\nreturns.columns = ['Amazon', 'Microsoft']\n\nLet’s take a minute to check the data we are working with. The plot below shows 3 things: 1. The returns of one stock agains the returns of the other by period, with a regression line computed with the whole period. 2. The returns of both stocks. 3. The log prices of both stocks. I took the log for comparative purposes only.\nWe can clearly see that a regression line fitted on the whole data is not a good model for all periods. This is one of the main weaknesses of linear models like the Capital Asset Pricing Model (CAPM).\n\n#collapse-hide\n# linear model\nregression = LinearRegression()\nregression.fit(X=returns['Amazon'].values.reshape(-1,1), y=returns['Microsoft'])\ny_pred = regression.predict(returns['Amazon'].values.reshape(-1,1))\n\n# code taken from [1]\nplt.figure(figsize=(15,6))\ncm = plt.get_cmap('jet')\ncolors = np.linspace(0.1, 1, len(returns))\nsc = plt.scatter(returns['Amazon'], returns['Microsoft'], c=colors, cmap=cm, edgecolor='k', alpha=0.6)\nplt.plot(returns['Amazon'].values, y_pred, color='red', label='Regression Line')\ncb = plt.colorbar(sc)\n\ncb.ax.set_yticklabels([str(p.date()) for p in returns[::len(returns) // 9].index])\nplt.xlabel('AMAZON');\nplt.ylabel('MICROSOFT');\nplt.grid();\nplt.show()\n\n# ---------\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,4));\n\nreturns.plot(ax=ax[0])\nax[0].grid();\nax[0].set_title('Returns');\nax[0].legend()\n\nnp.log(prices).plot(ax=ax[1])\nax[1].grid();\nax[1].set_title('Log (Prices)');\nax[1].legend();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow it is time to use the Kalman Filter. I’ve created my own Kalman library and that is what I am going to use. It is a multidimensional implementation, so we need to be careful when setting all the parameters.\nSince we want to estimate not only the beta but the alpha (intercept) too, we have to work in a 2-dimensional space; and that must hold for each time step.\nLet’s set the parameters 1 by 1, starting with the state transition model \\(A\\).\nThe state transition model \\(A\\) relates the state at the previous time step \\(k-1\\) to the state at the current step \\(k\\). Since stock returns can be considered random walks, \\(A\\) should be set to be the identity in order to model a martingale process. Notice how we create a 2x2 matrix for each time step.\n\nstock_a = returns['Amazon'].values\nstock_b = returns['Microsoft'].values\n\nn_steps = len(stock_a)\n\nA = np.array([np.eye(2)] * n_steps)\n\nNext, the control part. We are not going to use it, so we set everything to zero but taking into account the matrix dimensions we talked about earlier.\n\nB = np.zeros((n_steps, 2, 2)) # model\nU = np.zeros((n_steps, 2, 1)) # vector\n\nWe continue the filter design by looking at the initial estimates. The Kalman must start somewhere and we are in charge of defining that place. We assume the initial mean estimates as zero, both for alpha and beta, and the initial covariance estimates as 1, also both for alpha and beta.\n\nxk = np.array([0, 0]) # initial mean\nPk = np.ones((2, 2)) # initial covariance\n\nTo continue, we define the observed variable \\(Z\\) and the observation model \\(H\\). The observed variable is the stock a (we could also choose the stock b).\nThe observation matrix relates the state to the measurement \\(z_{k}\\). Remember the equation \\(z_{k} = H_{k} x_{k} + r_{k}\\)? By setting the \\(H_{k}\\) to be stock b at time \\(k\\) we have a nice linear regression; we just have to add ones to account for the intercept.\n\nIt does not matter if we stack the ones before or after, we would just need to take care of selecting the right series after computing everything.\n\n\nZ = stock_a.copy()\nH = np.expand_dims(np.vstack([[stock_b], [np.ones(len(stock_b))]]).T, axis=1)\n\nLastly, we define the noises \\(Q\\) and \\(R\\). Given that we fixed the observation covariance to be one, the smaller is \\(Q\\) the slower the beta will change. Notice how we are making \\(Q\\) a diagonal matrix for each time step.\nThere is no need to put special shape in \\(R\\) since we want to add 1 in each time step for all elements and NumPy will take care of the broadcasting, but we may need to check the dimensions in other cases.\n\nQ = np.array([1e-3 * np.eye(2)] * n_steps) # process noise / transition covariance\nR = np.ones(n_steps) # measurement noise / observation covariance\n\nIt is time to run the filter. The library works by passing all the parameters except \\(Z\\) and \\(U\\) to the constructor and later use the filter method with \\(Z\\) and \\(U\\) to get the estimated means and covariances.\n\nkf = KalmanFilter(A=A, xk=xk, B=B, Pk=Pk, H=H, Q=Q, R=R)\nstates, errors = kf.filter(Z=Z, U=U)\n\n\n#collapse-hide\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 6))\n\nax[0][0].plot(states[:, 0])\nax[0][0].set_title('Estimated means (beta)')\nax[0][0].grid()\n\nax[1][0].plot(states[:, 1])\nax[1][0].set_title('Estimated means (alpha / intercept)')\nax[1][0].grid()\n\nax[0][1].plot(errors[:, 0])\nax[0][1].set_title('Estimated covariances (beta)')\nax[0][1].grid()\n\nax[1][1].plot(errors[:, 1])\nax[1][1].set_title('Estimated covariances (alpha / intercept)')\nax[1][1].grid()\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#whats-next",
    "href": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#whats-next",
    "title": "Online beta estimation with the Kalman Filter",
    "section": "What’s next?",
    "text": "What’s next?\nNow you can use the estimated betas and alphas to perform a linear regression in an online fashion, updating the prior knowledge recursively with the observed variable.\n\n#collapse-hide\n# code taken from [1]\nplt.figure(figsize=(15,6))\ncm = plt.get_cmap('jet')\ncolors = np.linspace(0.1, 1, len(returns))\nsc = plt.scatter(returns['Amazon'], returns['Microsoft'], c=colors, cmap=cm, edgecolor='k', alpha=0.9)\nplt.plot(returns['Amazon'].values, y_pred, color='red', label='Regression Line')\ncb = plt.colorbar(sc)\n\n\nstep = 50\nspace = np.linspace(returns['Amazon'].min(), returns['Amazon'].max(), 2)\ncolors_l = np.linspace(0.1, 1, len(states[::step]))\nfor i, beta in enumerate(states[::step]):\n    plt.plot(space, beta[0] * space + beta[1], alpha=.2, lw=1, c=cm(colors_l[i]))\n\ncb.ax.set_yticklabels([str(p.date()) for p in returns[::len(returns) // 9].index])\nplt.xlabel('AMAZON');\nplt.ylabel('MICROSOFT');\nplt.grid();\nplt.show()\n\n\n\n\n\n\n\n\nThe dynamic beta and alpha provide a linear regression that adapts dynamically each time step. The regression line we created earlier is provided in red to compare. This looks much better!"
  },
  {
    "objectID": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#conclusion",
    "href": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#conclusion",
    "title": "Online beta estimation with the Kalman Filter",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we’ve seen how to perform online linear regression using the Kalman filter. The results are more realistic than a simple linear regression since they are computed updating prior distributions with new information.\nOne of the main drawbacks of the Kalman filter is that the assumptions of normality and linearity must hold in order for the filter to give us the optimal estimate [3].\nOverall, it is a very powerful and elegant technique that seems to outperform other models, like CAPM."
  },
  {
    "objectID": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#references",
    "href": "posts/2021_08_15_online_beta_with_kalman_filter_checkpoint.html#references",
    "title": "Online beta estimation with the Kalman Filter",
    "section": "References",
    "text": "References\n\n[1] Quantopian - Kalman Filters\n[2] Quantdare - The Kalman Filter\n[3] Thrun, S., Burgard, W., & Fox, D. (2006). Probabilistic robotics. Kybernetes.\n[4] Kevin P. Murphy - Machine Learning: A probabilistic perspective.\n[5] Jerome H. Friedman, Robert Tibshirani & Trevor Hastie - The Elements of Statistical Learning."
  },
  {
    "objectID": "posts/28_07_2024_index_tracking.html",
    "href": "posts/28_07_2024_index_tracking.html",
    "title": "Index Tracking: a naive approach",
    "section": "",
    "text": "We review in this post a naive approach to solve the Index Tracking problem using Principal Component Analysis.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nfrom sklearn.decomposition import PCA"
  },
  {
    "objectID": "posts/28_07_2024_index_tracking.html#introduction",
    "href": "posts/28_07_2024_index_tracking.html#introduction",
    "title": "Index Tracking: a naive approach",
    "section": "1. Introduction",
    "text": "1. Introduction\nA financial index is a number that represents the aggregate value of a group of items. In particular, a financial index is composed of a collection of assets, such as stocks or bonds (each instrument associated with a weight that determines its importance on the overall basket), which captures the value of a specific market or a segment of it (Benidis, K., Feng, Y., & Palomar, D. P. (2018)).\nIndex tracking is a well-known problem in passive investment that consists in designing a portfolio, called tracking portfolio, that closely follows a benchmark index. Usually, the tracking portfolio is constructed using a subset of the original financial instruments that compose the index.\n\n1.1. Tracking Error\nTo measure the goodness of a tracking portfolio with respect to the index we use the tracking error (TE). The TE is usually defined as the standard deviation of the difference between the returns of the tracking portfolio and the returns of the index\n\\[\nTE = Std(r_{index} - r_{tracking}),\n\\]\nwhere \\(r_{index}\\) are the returns of the index and \\(r_{tracking}\\) are the returns of the tracking portfolio, defined as\n\\[\nr_{tracking}=\\sum_{i}^{k} w_{i} \\cdot r_{i}.\n\\]\nHere, \\(w_{i}\\) and \\(r_{i}\\) are the weights and returns of asset \\(i\\) and \\(k\\) is the number of assets in the tracking portfolio."
  },
  {
    "objectID": "posts/28_07_2024_index_tracking.html#the-role-of-principal-component-analysis-in-index-tracking",
    "href": "posts/28_07_2024_index_tracking.html#the-role-of-principal-component-analysis-in-index-tracking",
    "title": "Index Tracking: a naive approach",
    "section": "2. The role of Principal Component Analysis in Index Tracking",
    "text": "2. The role of Principal Component Analysis in Index Tracking\nPrincipal Component Analysis (PCA) is a statistical technique that allows expressing a set of variables as a set of linearly uncorrelated variables called principal components. We are not going to dive into the definition and computations needed to perform PCA. Instead, we are going to focus on using this technique to select a subset of relevant instruments, from the total amount of instruments that compose and index, to construct a tracking portfolio."
  },
  {
    "objectID": "posts/28_07_2024_index_tracking.html#numerical-experiments",
    "href": "posts/28_07_2024_index_tracking.html#numerical-experiments",
    "title": "Index Tracking: a naive approach",
    "section": "3. Numerical Experiments",
    "text": "3. Numerical Experiments\nWe apply in this section the PCA-based index tracking methodology to the SP500. The idea is to generate a portfolio of size \\(k\\) every month. In actual fact, we are not truly rebalancing because we are not going to take into account the previous portfolio. We are also not going to take into account weight drift (or style drift) since we are not considering neither working with shares nor with a budget.\nThe following cells load the SP500 component prices and the index itself. Then, we compute the relative returns.\n\ndataset = pd.read_csv(\"data/sp500.csv\", index_col=0)\ndataset.index = pd.DatetimeIndex(dataset.index)\n\n# the index is the last column\ncomponents = dataset.iloc[:, :-1]\nindex = dataset.iloc[:, -1:]\n\nasset_returns = components.pct_change()\nindex_returns = index.pct_change().squeeze()\n\n\ndef get_portfolio_weights(\n    asset_returns,\n    computation_date,\n    portfolio_size,\n    lookback=pd.offsets.BusinessDay(252 * 2)\n):\n    \"\"\"Compute portfolio weights using PCA for a given portfolio size.\n\n    Returns are standardized using \"lookback\" periods, and the portfolio is\n    constructed by selecting the weights on the assets that contribute the most\n    to the variance.\n\n    Parameters\n    ----------\n    asset_returns : pandas.DataFrame\n        Pandas DataFrame containing the returns of the instruments that compose\n        the index.\n    computation_date : pandas.Timestamp\n        Date at which to perform the portfolio construction.\n    portfolio_size : int\n        Number of non-zero weights in the portfolio.\n    lookback : pandas.DateOffset\n        Number of days to consider to build the portfolio.\n\n    Returns\n    -------\n    weights : pandas.Series\n        A pandas Series of computed weights for the given computation date.\n    \"\"\"\n    ini = computation_date - lookback\n    end = computation_date\n\n    # subset returns to train interval and drop any asset with missing values\n    in_sample_asset_returns = asset_returns.loc[ini:end]\n    in_sample_asset_returns = in_sample_asset_returns.dropna(axis=1)\n\n    mean = in_sample_asset_returns.mean()\n    std = in_sample_asset_returns.std()\n\n    in_sample_asset_returns = (in_sample_asset_returns - mean) / std\n\n    model = PCA()\n    components = model.fit_transform(in_sample_asset_returns)\n    contributions = np.sum(np.abs(components), axis=0)\n    \n    # Select the top `cardinality` stocks based on their contributions\n    selected_indices = np.argsort(contributions)[-portfolio_size:]\n    \n    # Normalize the weights for the selected stocks\n    selected_contributions = contributions[selected_indices]\n    selected_weights = selected_contributions / np.sum(selected_contributions)\n    \n    # Create a full weight vector with zero weights for unselected stocks\n    full_weights = np.zeros(in_sample_asset_returns.shape[1])\n    full_weights[selected_indices] = selected_weights\n\n    weights = pd.Series(\n        full_weights,\n        index=in_sample_asset_returns.columns\n    )\n    weights = weights.reindex(asset_returns.columns).fillna(0)\n    weights.name = computation_date\n    weights = weights.to_frame()\n    \n    return weights\n\n\ndef get_portfolio_weights_at_different_dates(\n    asset_returns,\n    portfolio_size,\n    rebalance_dates=None,\n    lookback=pd.offsets.BusinessDay(252 * 2)\n):\n    \"\"\"Compute portfolio weights using PCA for different dates.\n\n    Returns are standardized using \"lookback\" periods, and the portfolio is\n    constructed by selecting the weights on the assets that contribute the most\n    to the variance.\n\n    Parameters\n    ----------\n    asset_returns : pandas.DataFrame\n        Pandas DataFrame containing the returns of the instruments that compose\n        the index.\n    portfolio_size : int\n        Number of non-zero weights in the portfolio.\n    rebalance_dates : list of pandas.Timestamp, optional, default: None\n        Dates at which to compute a new set of portfolio weights.\n    lookback : pandas.DateOffset\n        Number of days to consider to build the portfolio.\n\n    Returns\n    -------\n    weights_df : pandas.Series\n        A pandas DataFrame of computed weights for different computation dates.\n    \"\"\"\n    if rebalance_dates is None:\n        months = range(1, 13)\n        years = range(2010, 2025)\n        rebalance_dates = []\n        for m in months:\n            for y in years:\n                t = pd.Timestamp(year=y, month=m, day=1)\n                rebalance_dates.append(t)\n\n    # construct a portfolio for each date\n    weight_dict = {}\n    for computation_date in rebalance_dates:\n        _weight = get_portfolio_weights(\n            asset_returns=asset_returns,\n            computation_date=computation_date,\n            portfolio_size=portfolio_size,\n            lookback=lookback,\n        )\n        weight_dict[computation_date] = _weight.squeeze()\n\n    # reindex weights to have the same index as asset_returns\n    weights_df = pd.DataFrame(weight_dict).T\n    weights_df = weights_df.sort_index()\n    weights_df = weights_df.reindex(asset_returns.index, method=\"ffill\")\n\n    return weights_df\n\nWe compute a portfolio the first day of each month and year from 2010 to 2024\n\nmonths = range(1, 13)\nyears = range(2010, 2025)\n\nrebalance_dates = []\nfor m in months:\n    for y in years:\n        t = pd.Timestamp(year=y, month=m, day=1)\n        rebalance_dates.append(t)\n\nWe consider different portfolio sizes for the experiment. Theoretically, the bigger the portfolio the smaller the tracking error.\n\nportfolio_sizes = [5, 15, 30, 60, 100, 300]\n\nWe always work with the last two years of data, considering the last available date the portfolio computation date.\n\nweights_dict = {}\n\nfor k in portfolio_sizes:\n    weights_df = get_portfolio_weights_at_different_dates(\n        asset_returns=asset_returns,\n        portfolio_size=k,\n        rebalance_dates=rebalance_dates,\n        lookback=pd.offsets.BusinessDay(252 * 2)\n    )\n    weights_dict[k] = weights_df\n\nBelow we plot the rolling tracking error to evaluate this metric using a sliding-window approach. This allows us to asses the evolution of the tracking portfolio quality over time.\n\ndef get_tracking_error(asset_returns, index_returns, portfolio_weights_matrix):\n    tracking_portfolio = (asset_returns * portfolio_weights_matrix).sum(axis=1)\n    _index = index_returns.loc[tracking_portfolio.index].squeeze()\n\n    spread = tracking_portfolio - _index.loc[tracking_portfolio.index]\n    rolling_tracking_error = spread.rolling(252).std()\n    rolling_tracking_error *= np.sqrt(252) * 100\n    return rolling_tracking_error\n\n\nfig, ax = plt.subplots(1, 1, figsize=(17, 8))\n\nfor k, weights in weights_dict.items():\n    rolling_tracking_error = get_tracking_error(\n        asset_returns=asset_returns.loc[\"2010\":\"2024\"],\n        index_returns=index_returns.loc[\"2010\":\"2024\"],\n        portfolio_weights_matrix=weights.loc[\"2010\":\"2024\"]\n    )\n    rolling_tracking_error.plot(\n        ls=\"--\", marker=\".\", mfc=\"w\", lw=1, ax=ax, label=f\"Portfolio size: {k}\"\n    )\n\nax.grid(True, alpha=.3)\nax.legend(fontsize=16, ncol=2)\nax.set_title(\"Yearly rolling tracking error\", fontsize=23);\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\nax.set_ylabel(\"Yearly rolling TE\", fontsize=20)\nax.set_xlabel(\"Date\", fontsize=20)\nax.tick_params(axis='both', which='major', length=0, labelsize=20)\n\n\n\n\n\n\n\n\nAs expected, we clearly see the tracking error decreases with the size of the portfolio: the greater the number of elements the smaller the TE is. We notice significantly higher tracking errors at 2014, 2020 and 2023. The ones at 2020 and 2023 are caused by an increase in the volatility of the index, as shows below. The 2014 is probably due to the quality of the tracking not being very good, either because the PCA does not capture the dynamics or because of the covariance matrix used (in sample over the returns of the last 2 years) is significantly different from the matrix on the test period.\n\nfig, ax = plt.subplots(1, 1, figsize=(17, 8))\n\n(100 * np.sqrt(252) * index_returns.rolling(252).std()).loc[\"2010\":\"2024\"].plot(ax=ax)\n\nax.grid(True, alpha=.3)\nax.legend(fontsize=16, ncol=2)\nax.set_title(\"Yearly SP500 volatility\", fontsize=23);\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\nax.set_ylabel(\"Yearly volatility\", fontsize=20)\nax.set_xlabel(\"Date\", fontsize=20)\nax.tick_params(axis='both', which='major', length=0, labelsize=20)"
  },
  {
    "objectID": "posts/28_07_2024_index_tracking.html#why-is-it-called-naive",
    "href": "posts/28_07_2024_index_tracking.html#why-is-it-called-naive",
    "title": "Index Tracking: a naive approach",
    "section": "4. Why is it called “Naive”?",
    "text": "4. Why is it called “Naive”?\nThe approach we have used is far from desirable. Some reasons why: * PCA does not directly address the minimization of the tracking error. * PCA focuses on explaining the variance of the data, which may not be the best way to capture market movements. * Constraints: we have somewhat included the cardinality constraing on our portfolio construction but it was a far from elegant solution. What if we want to add more constraints? For intance: what if we want a portfolio computed at time \\(t+1\\) to be derived from the portfolio at time \\(t\\), limiting for example the number of assets that are allowed to change from \\(t\\) to \\(t+1\\)? PCA is not suited for these constraints."
  },
  {
    "objectID": "posts/28_07_2024_index_tracking.html#conclusions",
    "href": "posts/28_07_2024_index_tracking.html#conclusions",
    "title": "Index Tracking: a naive approach",
    "section": "5. Conclusions",
    "text": "5. Conclusions\nIn this post we have briefly review the definition of Index Tracking and we have applied Principal Component Analysis to construct tracking portfolios. We have tested the methodology using the SP500 dataset, for which we have computed portfolios from different sizes.\nResults show that we can achieve a 5% tracking error using portfolio sizes of 60 or more.\nFinally, we have stated that PCA is probably the most naive way to approach the Index Tracking problem, since it has many caviats."
  },
  {
    "objectID": "posts/28_07_2024_index_tracking.html#references",
    "href": "posts/28_07_2024_index_tracking.html#references",
    "title": "Index Tracking: a naive approach",
    "section": "References",
    "text": "References\n\nBenidis, K., Feng, Y., & Palomar, D. P. (2018). Optimization methods for financial index tracking: From theory to practice. Foundations and Trends® in Optimization, 3(3), 171-279."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "xy = lambda",
    "section": "",
    "text": "Index Tracking: a naive approach\n\n\n\n\n\n\nfinance\n\n\nanalysis\n\n\npython\n\n\nindex tracking\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nAlejandro Pérez Sanjuán\n\n\n\n\n\n\n\n\n\n\n\n\nOnline beta estimation with the Kalman Filter\n\n\n\n\n\n\nfinance\n\n\npython\n\n\ntracking\n\n\nbayes\n\n\nfiltering\n\n\n\n\n\n\n\n\n\nAug 15, 2021\n\n\nAlejandro Pérez Sanjuán\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Python to construct dollar indices\n\n\n\n\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\n\n\nJul 9, 2021\n\n\nAlejandro Pérez Sanjuán\n\n\n\n\n\n\nNo matching items"
  }
]